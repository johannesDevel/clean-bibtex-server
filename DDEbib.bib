@String{cgforum = "Computer Graphics Forum"}
@String{tog = "ACM TOG"}

  
  
@inproceedings{di_geronimo_surveying_2016,
	address = {Oulu, Finland},
	title = {Surveying personal device ecosystems with cross-device applications in mind},
	isbn = {978-1-4503-4366-4},
	url = {http://dl.acm.org/citation.cfm?doid=2914920.2915028},
	doi = {10.1145/2914920.2915028},
	abstract = {To inform the design of cross-device applications, it is important to know what devices users typically have within reach in various settings in the home, in the workplace and on the move. We report on a survey carried out using an online questionnaire and discuss the most signiﬁcant ﬁndings that should be taken into account in cross-device scenarios. The results are based on 293 participants covering a wide range of ages, nationalities, professions and living arrangements.},
	language = {en},
	urldate = {2019-03-11},
	booktitle = {Proceedings of the 5th {ACM} {International} {Symposium} on {Pervasive} {Displays} - {PerDis} '16},
	publisher = {ACM Press},
	author = {Di Geronimo, Linda and Husmann, Maria and Norrie, Moira C.},
	year = {2016},
	pages = {220--227},
	file = {Di Geronimo et al. - 2016 - Surveying personal device ecosystems with cross-de.pdf:files/77/Di Geronimo et al. - 2016 - Surveying personal device ecosystems with cross-de.pdf:application/pdf}
}

@article{langner_vistiles:_2018,
	title = {{VISTILES}: {Coordinating} and {Combining} {Co}-located {Mobile} {Devices} for {Visual} {Data} {Exploration}},
	volume = {24},
	issn = {1077-2626},
	shorttitle = {V\textsc{is}{T}\textsc{iles}},
	url = {http://ieeexplore.ieee.org/document/8017609/},
	doi = {10.1109/TVCG.2017.2744019},
	abstract = {We present VISTILES, a conceptual framework that uses a set of mobile devices to distribute and coordinate visualization views for the exploration of multivariate data. In contrast to desktop-based interfaces for information visualization, mobile devices offer the potential to provide a dynamic and user-deﬁned interface supporting co-located collaborative data exploration with different individual workﬂows. As part of our framework, we contribute concepts that enable users to interact with coordinated \& multiple views (CMV) that are distributed across several mobile devices. The major components of the framework are: (i) dynamic and ﬂexible layouts for CMV focusing on the distribution of views and (ii) an interaction concept for smart adaptations and combinations of visualizations utilizing explicit side-by-side arrangements of devices. As a result, users can beneﬁt from the possibility to combine devices and organize them in meaningful spatial layouts. Furthermore, we present a web-based prototype implementation as a speciﬁc instance of our concepts. This implementation provides a practical application case enabling users to explore a multivariate data collection. We also illustrate the design process including feedback from a preliminary user study, which informed the design of both the concepts and the ﬁnal prototype.},
	language = {en},
	number = {1},
	urldate = {2019-02-27},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Langner, Ricardo and Horak, Tom and Dachselt, Raimund},
	month = jan,
	year = {2018},
	pages = {626--636},
	file = {Langner et al. - 2018 - VISTILES Coordinating and Combining Co-located Mo.pdf:files/3/Langner et al. - 2018 - VISTILES Coordinating and Combining Co-located Mo.pdf:application/pdf}
}

@article{badam_visfer:_2019,
	title = {Visfer: {Camera}-based visual data transfer for cross-device visualization},
	volume = {18},
	issn = {1473-8716, 1473-8724},
	shorttitle = {Visfer},
	url = {http://journals.sagepub.com/doi/10.1177/1473871617725907},
	doi = {10.1177/1473871617725907},
	abstract = {Going beyond the desktop to leverage novel devices—such as smartphones, tablets, or large displays—for visual sensemaking typically requires supporting extraneous operations for device discovery, interaction sharing, and view management. Such operations can be time-consuming and tedious and distract the user from the actual analysis. Embodied interaction models in these multi-device environments can take advantage of the natural interaction and physicality afforded by multimodal devices and help effectively carry out these operations in visual sensemaking. In this article, we present cross-device interaction models for visualization spaces, that are embodied in nature, by conducting a user study to elicit actions from participants that could trigger a portrayed effect of sharing visualizations (and therefore information) across devices. We then explore one common interaction style from this design elicitation called Visfer, a technique for effortlessly sharing visualizations across devices using the visual medium. More specifically, this technique involves taking pictures of visualizations, or rather the QR codes augmenting them, on a display using the built-in camera on a handheld device. Our contributions include a conceptual framework for cross-device interaction and the Visfer technique itself, as well as transformation guidelines to exploit the capabilities of each specific device and a web framework for encoding visualization components into animated QR codes, which capture multiple frames of QR codes to embed more information. Beyond this, we also present the results from a performance evaluation for the visual data transfer enabled by Visfer. We end the article by presenting the application examples of our Visfer framework.},
	language = {en},
	number = {1},
	urldate = {2019-03-11},
	journal = {Information Visualization},
	author = {Badam, Sriram Karthik and Elmqvist, Niklas},
	month = jan,
	year = {2019},
	pages = {68--93},
	file = {Badam und Elmqvist - 2019 - Visfer Camera-based visual data transfer for cros.pdf:files/83/Badam und Elmqvist - 2019 - Visfer Camera-based visual data transfer for cros.pdf:application/pdf}
}

@article{desolda_exploring_2019,
	title = {Exploring spatially-aware cross-device interaction techniques for mobile collaborative sensemaking},
	volume = {122},
	issn = {10715819},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581918304786},
	doi = {10.1016/j.ijhcs.2018.08.006},
	abstract = {The collaborative decision-making process is traditionally supported by multi-user interfaces, such as large multi-touch screens or interactive tabletops for accessing, relating and comparing diﬀerent data sources. Since such multi-user interfaces are typically expensive and unavailable outside dedicated environments (e.g. labs, smart rooms), recent works have proposed “bring your own device” approaches that allow users to join their mobile devices (e.g. smartphones, tablets) in an ad-hoc manner to temporarily create multi-user cross-device systems. Such approaches can be enabled by spatially-aware cross-device interactions that have only been explored for simple operations. We conducted a three-step research study involving a total number of 65 users in 18 groups, in order to propose a composition paradigm that oﬀers three interaction techniques for performing more complex operations, such as forwarding to multiple devices queries or query results or aggregating and visualizing search results across device boundaries.},
	language = {en},
	urldate = {2019-03-11},
	journal = {International Journal of Human-Computer Studies},
	author = {Desolda, Giuseppe and Ardito, Carmelo and Jetter, Hans-Christian and Lanzilotti, Rosa},
	month = feb,
	year = {2019},
	pages = {1--20},
	file = {Desolda et al. - 2019 - Exploring spatially-aware cross-device interaction.pdf:files/92/Desolda et al. - 2019 - Exploring spatially-aware cross-device interaction.pdf:application/pdf}
}

@article{herrera_ping-pong:_2013,
	title = {Ping-{Pong}: {Using} smartphones to measure distances and relative positions},
	volume = {20},
	language = {en},
	author = {Herrera, Jorge and Kim, Hyung Suk},
	year = {2013},
	pages = {11},
	file = {Herrera und Kim - 2013 - Ping-Pong Using smartphones to measure distances .pdf:files/20/Herrera und Kim - 2013 - Ping-Pong Using smartphones to measure distances .pdf:application/pdf}
}

@article{huang_magmobile:_2012,
	title = {{MagMobile}: enhancing social interactions with rapid view-stitching games of mobile devices},
	abstract = {Most mobile games are designed for users to only focus on their own screens thus lack of face-to-face interaction even users are sitting together. Prior work shows that the shared information space created by multiple mobile devices can encourage users to communicate to each other naturally. The aim of this work is to provide a fluent view-stitching technique for mobile phone users to establish their information-shared view. We present MagMobile: a new spatial interaction technique that allows users to stitch views by simply putting multiple mobile devices close to each other. We describe the design of spatial-aware sensor module which is low cost and easy to be obtained into phones. We also propose two collaborative games to engage social interactions in the co-located place.},
	language = {en},
	author = {Huang, Da-Yuan and Lin, Chien-Pang and Hung, Yi-Ping and Chang, Tzu-Wen and Yu, Neng-Hao and Tsai, Min-Lun and Chen, Mike Y},
	year = {2012},
	pages = {4},
	file = {Huang et al. - MagMobile enhancing social interactions with rapi.pdf:files/29/Huang et al. - MagMobile enhancing social interactions with rapi.pdf:application/pdf}
}

@article{gronbaek_built-device_2016,
	title = {Built-{In} {Device} {Orientation} {Sensors} for {Ad}-{Hoc} {Pairing} and {Spatial} {Awareness}},
	abstract = {Mobile devices are equipped with multiple sensors. The ubiquity of these sensors is key in their ability to support in-the-wild application and use. Building on the ubiquity we look at how we can use this existing sensing infrastructure combined with user mediation to support ad-hoc sharing with nearby devices. In particular, the paper proposes a technique for exploiting the built-in compass in mobile devices to aid the process of pairing them for ad-hoc sharing in a variety of proxemic arrangements and F-formations.},
	language = {en},
	author = {Grønbæk, Jens Emil and O’Hara, Kenton},
	year = {2016},
	pages = {7},
	file = {Grønb und O’Hara - Built-In Device Orientation Sensors for Ad-Hoc Pai.pdf:files/11/Grønb und O’Hara - Built-In Device Orientation Sensors for Ad-Hoc Pai.pdf:application/pdf}
}

@inproceedings{hamilton_conductor:_2014,
	address = {Toronto, Ontario, Canada},
	title = {Conductor: enabling and understanding cross-device interaction},
	isbn = {978-1-4503-2473-1},
	shorttitle = {Conductor},
	url = {http://dl.acm.org/citation.cfm?doid=2556288.2557170},
	doi = {10.1145/2556288.2557170},
	abstract = {The proliferation of inexpensive connected devices has created a situation where a person, at any given moment, is surrounded by interactive computers. Despite this fact, there are very few means by which a user may take advantage of this large number of screens. We present Conductor, a prototype framework which serves as an exemplar for the construction of cross-device applications. We present a series of interaction methods by which users can easily share information, chain tasks across devices, and manage sessions across devices. We also present a cross-device usage scenario which utilizes several cross -device applications built within our prototype framework. We also describe a user study, which helped us to understand how users will take advantage of a large number of devices in support of performance of a sense making task.},
	language = {en},
	urldate = {2019-03-08},
	booktitle = {Proceedings of the 32nd annual {ACM} conference on {Human} factors in computing systems - {CHI} '14},
	publisher = {ACM Press},
	author = {Hamilton, Peter and Wigdor, Daniel J.},
	year = {2014},
	pages = {2773--2782},
	file = {Hamilton und Wigdor - 2014 - Conductor enabling and understanding cross-device.pdf:files/70/Hamilton und Wigdor - 2014 - Conductor enabling and understanding cross-device.pdf:application/pdf}
}

@inproceedings{boring_touch_2010,
	address = {Atlanta, Georgia, USA},
	title = {Touch projector: mobile interaction through video},
	isbn = {978-1-60558-929-9},
	shorttitle = {Touch projector},
	url = {http://portal.acm.org/citation.cfm?doid=1753326.1753671},
	doi = {10.1145/1753326.1753671},
	abstract = {In 1992, Tani et al. proposed remotely operating machines in a factory by manipulating a live video image on a computer screen. In this paper we revisit this metaphor and investigate its suitability for mobile use. We present Touch Projector, a system that enables users to interact with remote screens through a live video image on their mobile device. The handheld device tracks itself with respect to the surrounding displays. Touch on the video image is “projected” onto the target display in view, as if it had occurred there. This literal adaptation of Tani’s idea, however, fails because handheld video does not offer enough stability and control to enable precise manipulation. We address this with a series of improvements, including zooming and freezing the video image. In a user study, participants selected targets and dragged targets between displays using the literal and three improved versions. We found that participants achieved highest performance with automatic zooming and temporary image freezing.},
	language = {en},
	urldate = {2019-03-11},
	booktitle = {Proceedings of the 28th international conference on {Human} factors in computing systems - {CHI} '10},
	publisher = {ACM Press},
	author = {Boring, Sebastian and Baur, Dominikus and Butz, Andreas and Gustafson, Sean and Baudisch, Patrick},
	year = {2010},
	pages = {2287},
	file = {Boring et al. - 2010 - Touch projector mobile interaction through video.pdf:files/75/Boring et al. - 2010 - Touch projector mobile interaction through video.pdf:application/pdf}
}

@article{chung_visporter:_2014,
	title = {{VisPorter}: facilitating information sharing for collaborative sensemaking on multiple displays},
	volume = {18},
	issn = {1617-4909, 1617-4917},
	shorttitle = {{VisPorter}},
	url = {http://link.springer.com/10.1007/s00779-013-0727-2},
	doi = {10.1007/s00779-013-0727-2},
	abstract = {The multiplicity of computing and display devices currently available presents new opportunities for how visual analytics is performed. One of the signiﬁcant inherent challenges that comes with the use of multiple and varied types of displays for visual analytics is the sharing and subsequent integration of information among different devices. Multiple devices enable analysts to employ and extend visual space for working with visualizations, but this requires users to switch intermittently between activities and foci of interest over different workspaces. We present a visual analytics system, VisPorter, developed for use in a multiple display and device environment, and a user study that explores the usage and beneﬁts of this system. VisPorter enables seamless cross-device activity through lightweight touch interactions, and allows multiple displays and devices to be ﬂuidly connected for sensemaking.},
	language = {en},
	number = {5},
	urldate = {2019-03-08},
	journal = {Personal and Ubiquitous Computing},
	author = {Chung, Haeyong and North, Chris and Self, Jessica Zeitz and Chu, Sharon and Quek, Francis},
	month = jun,
	year = {2014},
	pages = {1169--1186},
	file = {Chung et al. - 2014 - VisPorter facilitating information sharing for co.pdf:files/72/Chung et al. - 2014 - VisPorter facilitating information sharing for co.pdf:application/pdf}
}

@inproceedings{wozniak_thaddeus:_2014,
	address = {Helsinki, Finland},
	title = {Thaddeus: a dual device interaction space for exploring information visualisation},
	isbn = {978-1-4503-2542-4},
	shorttitle = {Thaddeus},
	url = {http://dl.acm.org/citation.cfm?doid=2639189.2639237},
	doi = {10.1145/2639189.2639237},
	abstract = {This paper introduces Thaddeus—a mobile phone-tablet system for mobile interaction with information visualisations. Our work is motivated by the roles smartphones and tablets play in everyday interactive spaces as well as anticipated developments in mobile sensing technology. We also aim to meet the social challenges of a data-driven society. We designed and implemented a system that uses mutual spatial awareness as an input mode, producing new interaction patterns for mobile settings. We gathered extensive user insight from two design studies and evaluated the system in a controlled experiment. We used qualitative and quantitative measures in the final evaluation. The results show that the system does not have a significant impact on performance, but users perceive it as pleasurable and easy to use. Thaddeus offers an enhanced user experience when exploring information on the go, and provides insights for future designs of mobile multi-device systems.},
	language = {en},
	urldate = {2019-03-25},
	booktitle = {Proceedings of the 8th {Nordic} {Conference} on {Human}-{Computer} {Interaction} {Fun}, {Fast}, {Foundational} - {NordiCHI} '14},
	publisher = {ACM Press},
	author = {Woźniak, Paweł and Lischke, Lars and Schmidt, Benjamin and Zhao, Shengdong and Fjeld, Morten},
	year = {2014},
	pages = {41--50},
	file = {Woźniak et al. - 2014 - Thaddeus a dual device interaction space for expl.pdf:files/109/Woźniak et al. - 2014 - Thaddeus a dual device interaction space for expl.pdf:application/pdf}
}

@article{chung_savil:_2018,
	title = {{SAViL}: cross-display visual links for sensemaking in display ecologies},
	volume = {22},
	issn = {1617-4909, 1617-4917},
	shorttitle = {{SAViL}},
	url = {http://link.springer.com/10.1007/s00779-017-1091-4},
	doi = {10.1007/s00779-017-1091-4},
	abstract = {The main challenge associated with visual analysis using multiple displays is tied to the fact that a user must maintain awareness of and synthesize scattered information across separate displays—some of which may be out of the user’s immediate field of vision. To address this need, we present Spatially Aware Visual Links (SAViL), a crossdisplay visual link technique capable of (1) guiding the user’s attention to relevant information and (2) visually connecting related information across displays. In essence, SAViL visually represents the direct connections among different types of visual objects on separate displays to help users create semantic layers of documents spread over different displays. To test the efficacy of this system, we evaluated the impact of visual linking on the sensemaking process for text data utilizing multiple heterogeneous displays. The results of our evaluation indicate that cross-display links enable users to effectively forage for, organize, and synthesize relevant information scattered across multiple displays, integrating the different displays into a single cohesive visual workspace to support their sensemaking tasks.},
	language = {en},
	number = {2},
	urldate = {2019-03-05},
	journal = {Personal and Ubiquitous Computing},
	author = {Chung, Haeyong and North, Chris},
	month = apr,
	year = {2018},
	pages = {409--431},
	file = {Chung und North - 2018 - SAViL cross-display visual links for sensemaking .pdf:files/48/Chung und North - 2018 - SAViL cross-display visual links for sensemaking .pdf:application/pdf}
}

@inproceedings{lucero_collaborative_2010,
	address = {Lisbon, Portugal},
	title = {Collaborative use of mobile phones for brainstorming},
	isbn = {978-1-60558-835-3},
	url = {http://portal.acm.org/citation.cfm?doid=1851600.1851659},
	doi = {10.1145/1851600.1851659},
	abstract = {Mobile phones have traditionally been utilized for personal and individual use. In this paper we explore shared co-located interactions with mobile phones. We introduce a phone-based application that supports ad hoc brainstorming sessions. The prototype allows a workgroup to create, edit and view virtual mind-map notes on any table surface. The prototype encourages people to use the devices interchangeably and thus engage in social interactions. Evaluations show that participants were able to easily create mind maps and that the prototype supports different strategies in mind-map creation.},
	language = {en},
	urldate = {2019-02-27},
	booktitle = {Proceedings of the 12th international conference on {Human} computer interaction with mobile devices and services - {MobileHCI} '10},
	publisher = {ACM Press},
	author = {Lucero, Andrés and Keränen, Jaakko and Korhonen, Hannu},
	year = {2010},
	pages = {337},
	file = {Lucero et al. - 2010 - Collaborative use of mobile phones for brainstormi.pdf:files/27/Lucero et al. - 2010 - Collaborative use of mobile phones for brainstormi.pdf:application/pdf}
}

@inproceedings{schreiner_connichiwa:_2015,
	address = {Seoul, Republic of Korea},
	title = {Connichiwa: {A} {Framework} for {Cross}-{Device} {Web} {Applications}},
	isbn = {978-1-4503-3146-3},
	shorttitle = {Connichiwa},
	url = {http://dl.acm.org/citation.cfm?doid=2702613.2732909},
	doi = {10.1145/2702613.2732909},
	abstract = {While Mark Weiser's vision of ubiquitous computing is getting closer to reality, a fundamental part of it - the interconnection of devices into a "ubiquitous network" is not achieved yet. Differences in hardware, architecture, and missing standardizations are just some reasons for this. We think that existing research is not versatile enough and too tailored to either single applications, hardware, or location. We contribute Connichiwa – a versatile framework for creating web applications across multiple devices. We base Connichiwa on four key goals: integration of existing devices, independence of network infrastructure, versatility of application scenario, and usability of its API. Connichiwa runs web applications on off-the-shelf consumer devices. With no external dependencies, such as a server, it enables a great variety of possible scenarios. We tested the technical feasibility of Connichiwa in seven example applications and plan to evaluate the framework and the usability of its API in a one-week Hackathon.},
	language = {en},
	urldate = {2019-02-27},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems} - {CHI} {EA} '15},
	publisher = {ACM Press},
	author = {Schreiner, Mario and Rädle, Roman and Jetter, Hans-Christian and Reiterer, Harald},
	year = {2015},
	pages = {2163--2168},
	file = {Schreiner et al. - 2015 - Connichiwa A Framework for Cross-Device Web Appli.pdf:files/16/Schreiner et al. - 2015 - Connichiwa A Framework for Cross-Device Web Appli.pdf:application/pdf}
}

@inproceedings{langner_demonstrating_2018,
	address = {Castiglione della Pescaia, Grosseto, Italy},
	title = {Demonstrating vistiles: visual data exploration using mobile devices},
	isbn = {978-1-4503-5616-9},
	shorttitle = {Demonstrating vistiles},
	url = {http://dl.acm.org/citation.cfm?doid=3206505.3206583},
	doi = {10.1145/3206505.3206583},
	abstract = {We demonstrate the prototype of the conceptual VisTiles framework. VisTiles allows exploring multivariate data sets by using multiple coordinated views that are distributed across a set of mobile devices. This setup allows users to benefit from dynamic and user-defined interface arrangements and to easily initiate co-located data exploration sessions. The current web-based prototype runs on commodity devices and is able to determine the spatial device arrangement by either a cross-device pinch gesture or an external tracking system. Multiple data sets are provided that can be explored by different visualizations (e.g., scatterplots, parallel coordinate plots, stream graphs). With this demonstration, we showcase the general concepts of VisTiles and discuss ideas for enhancements as well the potential for application cases beyond data analysis.},
	language = {en},
	urldate = {2019-02-27},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Advanced} {Visual} {Interfaces}  - {AVI} '18},
	publisher = {ACM Press},
	author = {Langner, Ricardo and Horak, Tom and Dachselt, Raimund},
	year = {2018},
	pages = {1--3},
	file = {Langner et al. - 2018 - Demonstrating vistiles visual data exploration us.pdf:files/5/Langner et al. - 2018 - Demonstrating vistiles visual data exploration us.pdf:application/pdf}
}

@inproceedings{strohmeier_displaypointers:_2015,
	address = {Iskandar, Malaysia},
	title = {{DisplayPointers}: seamless cross-device interactions},
	isbn = {978-1-4503-3852-3},
	shorttitle = {{DisplayPointers}},
	url = {http://dl.acm.org/citation.cfm?doid=2832932.2832958},
	doi = {10.1145/2832932.2832958},
	abstract = {We present a system for cross-device interactions and interaction scenarios based on touch events between devices. DisplayPointers were designed to explore the affordances of manipulating physical display-objects in multi-device environments. The interactions presented in this paper are an exploration of what a touch between two devices means; a search for a language of physical cross-device interactions. DisplayPointers are implemented by augmenting off-theshelf devices: our system is fully mobile and can be easily implemented by other researchers and designers.},
	language = {en},
	urldate = {2019-03-11},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Advances} in {Computer} {Entertainment} {Technology} - {ACE} '15},
	publisher = {ACM Press},
	author = {Strohmeier, Paul},
	year = {2015},
	pages = {1--8},
	file = {Strohmeier - 2015 - DisplayPointers seamless cross-device interaction.pdf:files/81/Strohmeier - 2015 - DisplayPointers seamless cross-device interaction.pdf:application/pdf}
}

@inproceedings{ohta_dynamically_2008,
	address = {Yokohama, Japan},
	title = {Dynamically reconfigurable multi-display environment for {CG} contents},
	isbn = {978-1-60558-393-8},
	url = {http://portal.acm.org/citation.cfm?doid=1501750.1501866},
	doi = {10.1145/1501750.1501866},
	abstract = {We are designing an application framework for creating graphics contents that utilize a multi-display environment. The processes store and exchange the geometry and event information via a shared space. We use a sensor to detect the placement of the displays. This enables the environment to change the number and layout of its displays dynamically.},
	language = {en},
	urldate = {2019-03-05},
	booktitle = {Proceedings of the 2008 {International} {Conference} in {Advances} on {Computer} {Entertainment} {Technology} - {ACE} '08},
	publisher = {ACM Press},
	author = {Ohta, Takashi},
	year = {2008},
	pages = {416},
	file = {Ohta - 2008 - Dynamically reconfigurable multi-display environme.pdf:files/40/Ohta - 2008 - Dynamically reconfigurable multi-display environme.pdf:application/pdf}
}

@inproceedings{hardy_touch_2008,
	address = {Amsterdam, The Netherlands},
	title = {Touch \& interact: touch-based interaction of mobile phones with displays},
	isbn = {978-1-59593-952-4},
	shorttitle = {Touch \& interact},
	url = {http://portal.acm.org/citation.cfm?doid=1409240.1409267},
	doi = {10.1145/1409240.1409267},
	abstract = {The limited screen size and resolution of current mobile devices can still be problematic for map, multimedia and browsing applications. In this paper we present Touch \& Interact: an interaction technique in which a mobile phone is able to touch a display, at any position, to perform selections. Through the combination of the output capabilities of the mobile phone and display, applications can share the entire display space. Moreover, there is potential to realize new interaction techniques between the phone and display. For example, select \& pick and select \& drop are interactions whereby entities can be picked up onto the phone or dropped onto the display. We report the implementation of Touch \& Interact, its usage for a tourist guide application and experimental comparison. The latter shows that the performance of Touch \& Interact is comparable to approaches based on a touch screen; it also shows the advantages of our system regarding ease of use, intuitiveness and enjoyment.},
	language = {en},
	urldate = {2019-03-05},
	booktitle = {Proceedings of the 10th international conference on {Human} computer interaction with mobile devices and services - {MobileHCI} '08},
	publisher = {ACM Press},
	author = {Hardy, Robert and Rukzio, Enrico},
	year = {2008},
	pages = {245},
	file = {Hardy und Rukzio - 2008 - Touch & interact touch-based interaction of mobil.pdf:files/42/Hardy und Rukzio - 2008 - Touch & interact touch-based interaction of mobil.pdf:application/pdf}
}

@inproceedings{fei_peripheral_2013,
	address = {St. Andrews, Scotland, United Kingdom},
	title = {Peripheral array of tangible {NFC} tags: positioning portals for embodied trans-surface interaction},
	isbn = {978-1-4503-2271-3},
	shorttitle = {Peripheral array of tangible {NFC} tags},
	url = {http://dl.acm.org/citation.cfm?doid=2512349.2512820},
	doi = {10.1145/2512349.2512820},
	abstract = {Trans-surface interaction addresses moving information objects across multi-display environments that support sensory interaction modalities such as touch, pen, and free-air. Embodiment means using spatial relationships among surfaces and human bodies to facilitate users’ understanding of interaction. In the present embodied trans-surface interaction technique, a peripheral NFC tag array provides tangible affordances for connecting mobile devices to positions on a collaborative surface. Touching a tag initiates a trans-surface portal. Each portal visually associates a mobile device and its user with a place on the collaborative surface. The portal’s manifestation at the top of the mobile device supports ‘ﬂicking over’ interaction, like playing cards. The technique is simple, inexpensive, reliable, scalable, and generally applicable for co-located collaboration. We developed a co-located collaborative rich information prototype to demonstrate the embodied trans-surface interaction technique and support imagining and planning tasks.},
	language = {en},
	urldate = {2019-03-05},
	booktitle = {Proceedings of the 2013 {ACM} international conference on {Interactive} tabletops and surfaces - {ITS} '13},
	publisher = {ACM Press},
	author = {Fei, Shenfeng and Webb, Andrew M. and Kerne, Andruid and Qu, Yin and Jain, Ajit},
	year = {2013},
	pages = {33--36},
	file = {Fei et al. - 2013 - Peripheral array of tangible NFC tags positioning.pdf:files/38/Fei et al. - 2013 - Peripheral array of tangible NFC tags positioning.pdf:application/pdf}
}

@misc{gapminder2019, 
	title={Gapminder},
	url={https://www.gapminder.org/},
	journal={Gapminder}
}

@article{tobiasz2009lark,
	title={Lark: Coordinating co-located collaboration with information visualization},
	author={Tobiasz, Matthew and Isenberg, Petra and Carpendale, Sheelagh},
	journal={IEEE transactions on visualization and computer graphics},
	volume={15},
	number={6},
	pages={1065--1072},
	year={2009},
	publisher={IEEE}
}

@article{sadana_designing_2016,
	title = {Designing {Multiple} {Coordinated} {Visualizations} for {Tablets}},
	volume = {35},
	issn = {01677055},
	url = {http://doi.wiley.com/10.1111/cgf.12902},
	doi = {10.1111/cgf.12902},
	abstract = {The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.},
	language = {en},
	number = {3},
	urldate = {2019-03-25},
	journal = {Computer Graphics Forum},
	author = {Sadana, R. and Stasko, J.},
	month = jun,
	year = {2016},
	pages = {261--270},
	file = {Sadana und Stasko - 2016 - Designing Multiple Coordinated Visualizations for .pdf:files/103/Sadana und Stasko - 2016 - Designing Multiple Coordinated Visualizations for .pdf:application/pdf}
}

@inproceedings{goel2014surfacelink,
  title={SurfaceLink: using inertial and acoustic sensing to enable multi-device interaction on a surface},
  author={Goel, Mayank and Lee, Brendan and Islam Aumi, Md Tanvir and Patel, Shwetak and Borriello, Gaetano and Hibino, Stacie and Begole, Bo},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages={1387--1396},
  year={2014}
}

@inproceedings{hinckley2004stitching,
  title={Stitching: pen gestures that span multiple displays},
  author={Hinckley, Ken and Ramos, Gonzalo and Guimbretiere, Francois and Baudisch, Patrick and Smith, Marc},
  booktitle={Proceedings of the working conference on Advanced visual interfaces},
  pages={23--31},
  year={2004}
}

@inproceedings{tandler2001connectables,
  title={Connectables: dynamic coupling of displays for the flexible creation of shared workspaces},
  author={Tandler, Peter and Prante, Thorsten and M{\"u}ller-Tomfelde, Christian and Streitz, Norbert and Steinmetz, Ralf},
  booktitle={Proceedings of the 14th annual ACM symposium on User interface software and technology},
  pages={11--20},
  year={2001}
}

@inproceedings{grubert2017towards,
  title={Towards ad hoc mobile multi-display environments on commodity mobile devices},
  author={Grubert, Jens and Kr{\"a}nz, Matthias},
  booktitle={2017 IEEE Virtual Reality (VR)},
  pages={461--462},
  year={2017},
  organization={IEEE}
}

@inproceedings{rekimoto1999augmented,
  title={Augmented surfaces: a spatially continuous work space for hybrid computing environments},
  author={Rekimoto, Jun and Saitoh, Masanori},
  booktitle={Proceedings of the SIGCHI conference on Human Factors in Computing Systems},
  pages={378--385},
  year={1999}
}

@article{spindler2014tangible,
  title={Tangible displays for the masses: spatial interaction with handheld displays by using consumer depth cameras},
  author={Spindler, Martin and B{\"u}schel, Wolfgang and Winkler, Charlotte and Dachselt, Raimund},
  journal={Personal and Ubiquitous Computing},
  volume={18},
  number={5},
  pages={1213--1225},
  year={2014},
  publisher={Springer}
}

@inproceedings{grubert2017headphones,
  title={Headphones: Ad hoc mobile multi-display environments through head tracking},
  author={Grubert, Jens and Kranz, Matthias},
  booktitle={Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  pages={3966--3971},
  year={2017}
}

@inproceedings{hincapie2015gyrowand,
  title={GyroWand: IMU-based raycasting for augmented reality head-mounted displays},
  author={Hincapi{\'e}-Ramos, Juan David and Ozacar, Kasim and Irani, Pourang P and Kitamura, Yoshifumi},
  booktitle={Proceedings of the 3rd ACM Symposium on Spatial User Interaction},
  pages={89--98},
  year={2015}
}